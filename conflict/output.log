nohup: å¿½ç•¥è¾“å…¥
ONLINE
CURMODELcognitivecomputations/dolphin-2.9-llama3-8b
INFO 12-09 21:26:51 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 12-09 21:26:51 config.py:1020] Defaulting to use mp for distributed inference
INFO 12-09 21:26:51 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='cognitivecomputations/dolphin-2.9-llama3-8b', speculative_config=None, tokenizer='cognitivecomputations/dolphin-2.9-llama3-8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir='mnt/data/haochuntang/hfllm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=cognitivecomputations/dolphin-2.9-llama3-8b, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
WARNING 12-09 21:26:52 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 12-09 21:26:52 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 12-09 21:26:53 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:26:53 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:26:53 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:26:53 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:26:53 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:26:53 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:26:53 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:26:53 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:26:53 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:26:53 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:26:53 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:26:53 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:26:53 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:26:53 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:26:53 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
INFO 12-09 21:26:55 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:26:55 utils.py:961] Found nccl from library libnccl.so.2
INFO 12-09 21:26:55 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:26:55 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:26:55 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:26:55 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:26:55 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:26:55 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:26:55 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:26:55 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:26:55 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:26:55 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:26:55 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:26:55 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:26:55 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:26:55 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=258247)[0;0m WARNING 12-09 21:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=258246)[0;0m WARNING 12-09 21:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=258248)[0;0m WARNING 12-09 21:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 12-09 21:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=258249)[0;0m WARNING 12-09 21:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=258251)[0;0m WARNING 12-09 21:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=258252)[0;0m WARNING 12-09 21:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=258250)[0;0m WARNING 12-09 21:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 12-09 21:26:56 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f6a6b12c9b0>, local_subscribe_port=34695, remote_subscribe_port=None)
INFO 12-09 21:26:56 model_runner.py:1072] Starting to load model cognitivecomputations/dolphin-2.9-llama3-8b...
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:26:56 model_runner.py:1072] Starting to load model cognitivecomputations/dolphin-2.9-llama3-8b...
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:26:56 model_runner.py:1072] Starting to load model cognitivecomputations/dolphin-2.9-llama3-8b...
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:26:56 model_runner.py:1072] Starting to load model cognitivecomputations/dolphin-2.9-llama3-8b...
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:26:56 model_runner.py:1072] Starting to load model cognitivecomputations/dolphin-2.9-llama3-8b...
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:26:56 model_runner.py:1072] Starting to load model cognitivecomputations/dolphin-2.9-llama3-8b...
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:26:56 model_runner.py:1072] Starting to load model cognitivecomputations/dolphin-2.9-llama3-8b...
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:26:56 model_runner.py:1072] Starting to load model cognitivecomputations/dolphin-2.9-llama3-8b...
INFO 12-09 21:26:57 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:26:57 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:26:57 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:26:57 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:26:57 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:26:57 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:26:57 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:26:57 weight_utils.py:243] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.78it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  3.21it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.33it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.69it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.71it/s]

INFO 12-09 21:26:59 model_runner.py:1077] Loading model weights took 1.8735 GB
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:26:59 model_runner.py:1077] Loading model weights took 1.8735 GB
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:27:00 model_runner.py:1077] Loading model weights took 1.8735 GB
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:27:00 model_runner.py:1077] Loading model weights took 1.8735 GB
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:27:00 model_runner.py:1077] Loading model weights took 1.8735 GB
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:27:01 model_runner.py:1077] Loading model weights took 1.8735 GB
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:27:01 model_runner.py:1077] Loading model weights took 1.8735 GB
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:27:02 model_runner.py:1077] Loading model weights took 1.8735 GB
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:27:04 worker.py:232] Memory profiling results: total_gpu_memory=47.53GiB initial_memory_usage=2.34GiB peak_torch_memory=2.02GiB memory_usage_post_profile=2.44GiB non_torch_memory=0.56GiB kv_cache_size=40.20GiB gpu_memory_utilization=0.90
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:27:04 worker.py:232] Memory profiling results: total_gpu_memory=47.53GiB initial_memory_usage=2.36GiB peak_torch_memory=2.02GiB memory_usage_post_profile=2.49GiB non_torch_memory=0.61GiB kv_cache_size=40.15GiB gpu_memory_utilization=0.90
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:27:04 worker.py:232] Memory profiling results: total_gpu_memory=47.53GiB initial_memory_usage=2.36GiB peak_torch_memory=2.02GiB memory_usage_post_profile=2.49GiB non_torch_memory=0.61GiB kv_cache_size=40.15GiB gpu_memory_utilization=0.90
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:27:04 worker.py:232] Memory profiling results: total_gpu_memory=47.53GiB initial_memory_usage=2.34GiB peak_torch_memory=2.02GiB memory_usage_post_profile=2.44GiB non_torch_memory=0.56GiB kv_cache_size=40.20GiB gpu_memory_utilization=0.90
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:27:04 worker.py:232] Memory profiling results: total_gpu_memory=47.53GiB initial_memory_usage=2.36GiB peak_torch_memory=2.02GiB memory_usage_post_profile=2.49GiB non_torch_memory=0.61GiB kv_cache_size=40.14GiB gpu_memory_utilization=0.90
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:27:04 worker.py:232] Memory profiling results: total_gpu_memory=47.53GiB initial_memory_usage=2.34GiB peak_torch_memory=2.02GiB memory_usage_post_profile=2.45GiB non_torch_memory=0.56GiB kv_cache_size=40.19GiB gpu_memory_utilization=0.90
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:27:04 worker.py:232] Memory profiling results: total_gpu_memory=47.53GiB initial_memory_usage=2.36GiB peak_torch_memory=2.02GiB memory_usage_post_profile=2.49GiB non_torch_memory=0.61GiB kv_cache_size=40.14GiB gpu_memory_utilization=0.90
INFO 12-09 21:27:04 worker.py:232] Memory profiling results: total_gpu_memory=47.53GiB initial_memory_usage=2.34GiB peak_torch_memory=3.08GiB memory_usage_post_profile=2.50GiB non_torch_memory=0.62GiB kv_cache_size=39.08GiB gpu_memory_utilization=0.90
INFO 12-09 21:27:04 distributed_gpu_executor.py:57] # GPU blocks: 160074, # CPU blocks: 16384
INFO 12-09 21:27:04 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 625.29x
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:27:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:27:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:27:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:27:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:27:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:27:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:27:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:27:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 12-09 21:27:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 12-09 21:27:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:27:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:27:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:27:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:27:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:27:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:27:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=258249)[0;0m INFO 12-09 21:27:20 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.43 GiB
[1;36m(VllmWorkerProcess pid=258250)[0;0m INFO 12-09 21:27:20 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.43 GiB
[1;36m(VllmWorkerProcess pid=258248)[0;0m INFO 12-09 21:27:20 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.43 GiB
[1;36m(VllmWorkerProcess pid=258247)[0;0m INFO 12-09 21:27:20 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.43 GiB
[1;36m(VllmWorkerProcess pid=258251)[0;0m INFO 12-09 21:27:20 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.43 GiB
[1;36m(VllmWorkerProcess pid=258246)[0;0m INFO 12-09 21:27:20 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.43 GiB
[1;36m(VllmWorkerProcess pid=258252)[0;0m INFO 12-09 21:27:20 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.43 GiB
INFO 12-09 21:27:20 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.43 GiB
CURMODELallenai/Llama-3.1-Tulu-3-8B-SFT
INFO 12-09 21:36:03 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 12-09 21:36:03 config.py:1020] Defaulting to use mp for distributed inference
INFO 12-09 21:36:03 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='allenai/Llama-3.1-Tulu-3-8B-SFT', speculative_config=None, tokenizer='allenai/Llama-3.1-Tulu-3-8B-SFT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir='mnt/data/haochuntang/hfllm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/Llama-3.1-Tulu-3-8B-SFT, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/haochuntang/llm-fingerprint/conflict/evaluation.py", line 486, in <module>
[rank0]:     main(SAMPLE_SIZE,"online")
[rank0]:   File "/home/haochuntang/llm-fingerprint/conflict/evaluation.py", line 462, in main
[rank0]:     extracted_answers = evaluate_from_sample(sampled_indices,True)
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/llm-fingerprint/conflict/evaluation.py", line 232, in evaluate_from_sample
[rank0]:     model, tokenizer = load_model_HF(model_name)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/llm-fingerprint/conflict/evaluation.py", line 47, in load_model_HF
[rank0]:     llm = LLM(model=model_name, gpu_memory_utilization=float(0.9),
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/utils.py", line 1028, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 210, in __init__
[rank0]:     self.llm_engine = self.engine_class.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 585, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 320, in __init__
[rank0]:     self.tokenizer = self._init_tokenizer()
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 629, in _init_tokenizer
[rank0]:     return init_tokenizer_from_configs(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer_group/__init__.py", line 33, in init_tokenizer_from_configs
[rank0]:     return get_tokenizer_group(parallel_config.tokenizer_pool_config,
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer_group/__init__.py", line 54, in get_tokenizer_group
[rank0]:     return tokenizer_cls.from_config(tokenizer_pool_config, **init_kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer_group/tokenizer_group.py", line 30, in from_config
[rank0]:     return cls(**init_kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer_group/tokenizer_group.py", line 23, in __init__
[rank0]:     self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py", line 165, in get_tokenizer
[rank0]:     raise e
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py", line 144, in get_tokenizer
[rank0]:     tokenizer = AutoTokenizer.from_pretrained(
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 844, in from_pretrained
[rank0]:     tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 698, in get_tokenizer_config
[rank0]:     result = json.load(reader)
[rank0]:              ^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/json/__init__.py", line 293, in load
[rank0]:     return loads(fp.read(),
[rank0]:            ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/json/__init__.py", line 346, in loads
[rank0]:     return _default_decoder.decode(s)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/json/decoder.py", line 337, in decode
[rank0]:     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haochuntang/miniconda3/envs/finger/lib/python3.12/json/decoder.py", line 355, in raw_decode
[rank0]:     raise JSONDecodeError("Expecting value", s, err.value) from None
[rank0]: json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
ERROR 12-09 21:36:04 multiproc_worker_utils.py:116] Worker VllmWorkerProcess pid 258250 died, exit code: -15
INFO 12-09 21:36:04 multiproc_worker_utils.py:120] Killing local vLLM worker processes
[rank0]:[W1209 21:36:09.299971420 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/haochuntang/miniconda3/envs/finger/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
