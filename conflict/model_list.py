MODEL_LIST = [


              "/mnt/data/yuliangyan/meta-llama/Meta-Llama-3-8B",
              "/mnt/data/yuliangyan/instruction_tuning_models/llama3-ft",

              # "/mnt/data/yuliangyan/meta-llama/Meta-Llama-3-8B-Instruct",
              # "/mnt/data/yuliangyan/instruction_tuning_models/llama3-instruct-ft",

              # "/mnt/data/yuliangyan/meta-llama/Meta-Llama-3.1-8B",
              # "/mnt/data/yuliangyan/instruction_tuning_models/llama31-ft",

              # "/mnt/data/yuliangyan/meta-llama/Meta-Llama-3.1-8B-Instruct",
              # "/mnt/data/yuliangyan/instruction_tuning_models/llama31-instruct-ft",

              "/mnt/data/yuliangyan/mistralai/Mistral-7B-v0.1",
              "/mnt/data/yuliangyan/instruction_tuning_models/mistral-ft",

              "/mnt/data/yuliangyan/deepseek-ai/deepseek-llm-7b-base",
              "/mnt/data/yuliangyan/instruction_tuning_models/deepseek-ft",

              # "/mnt/data/yuliangyan/deepseek-ai/deepseek-llm-7b-chat",
              # "/mnt/data/yuliangyan/instruction_tuning_models/deepseek-chat-ft",

              # "/mnt/data/yuliangyan/deepseek-ai/deepseek-math-7b-instruct",
              # "/mnt/data/yuliangyan/instruction_tuning_models/deepseek-math-instruct-ft",

              "/mnt/data/yuliangyan/Qwen/Qwen2.5-7B",
              "/mnt/data/yuliangyan/instruction_tuning_models/qwen25-ft",

              # "/mnt/data/yuliangyan/microsoft/Phi-3-medium-4k-instruct",
              # "/mnt/data/yuliangyan/instruction_tuning_models/phi3-instruct-ft",

              ]
MODEL_LABEL = ["llama3",
               "llama3-ft",
               "llama3-instruct",
               "llama3-instruct-ft",
               "llama3.1",
               "llama3.1-ft",
               "llama3.1-instruct",
               "llama3.1-instruct-ft",
               "mistral",
               "mistral-ft",
               "deepseek",
               "deepseek-ft",
               "deepseek-chat",
               "deepseek-chat-ft",
               "deepseek-math",
               "deepseek-math-ft",
               "qwen2.5",
               "qwen2.5-ft",
               "phi3",
               "phi3-ft",

              #  "gemma2",
              #  "gemma2-ft"
               ]
# TODO
# gemmma-2
HF_MODEL_LIST = [
    # "NousResearch/Hermes-3-Llama-3.1-8B"
    # "akjindal53244/Llama-3.1-Storm-8B"
    # "meta-llama/Llama-3.1-8B-Instruct",
    # "cognitivecomputations/dolphin-2.9-llama3-8b",
    # "allenai/Llama-3.1-Tulu-3-8B-SFT",
    # "MLP-KTLim/llama-3-Korean-Bllossom-8B",
    "Groq/Llama-3-Groq-8B-Tool-Use",
    # "aaditya/Llama3-OpenBioLLM-8B",
    # "openchat/openchat-3.6-8b-20240522",
    "ruslanmv/Medical-Llama3-8B",
    "Weyaxi/Einstein-v6.1-Llama3-8B",
    # "rinna/llama-3-youko-8b",
    # "mlabonne/OrpoLlama-3-8B",
    # "/mnt/data/haochuntang/hfllm_cache/models--ytu-ce-cosmos--Turkish-Llama-8b-v0.1",
    # "/mnt/data/haochuntang/hfllm_cache/models--Groq--Llama-3-Groq-8B-Tool-Use",
    # "/mnt/data/haochuntang/hfllm_cache/models--MLP-KTLim--llama-3-Korean-Bllossom-8B",
    # "/mnt/data/haochuntang/hfllm_cache/models--ruslanmv--Medical-Llama3-8B",
    # "/mnt/data/haochuntang/hfllm_cache/models--Weyaxi--Einstein-v6.1-Llama3-8B",
    # "/mnt/data/haochuntang/hfllm_cache/models--ytu-ce-cosmos--Turkish-Llama-8b-v0.1",
              ]